{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71decfca-a0bc-47f3-a438-77af03cbfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import correctionlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9511686-5f2f-4376-a6c1-b7d1993455d2",
   "metadata": {},
   "source": [
    "## Read CSV files\n",
    "\n",
    "First, we will open the .csv files we saved during the event selection process and store all the data in a dictionary. We will also create a dictionary to hold the number of events for each sample that we found in yesterday's background modeling lesson. The keys of these dictionaries will be the types of data samples we are using in this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985049b5-d907-43ff-943e-27937d595bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {}\n",
    "datadict['signal'] = np.genfromtxt('SUMMED_signal_M2000.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['tt_semilep'] = np.genfromtxt('SUMMED_ttsemilep.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['tt_had'] = np.genfromtxt('SUMMED_tthadronic.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['tt_lep'] = np.genfromtxt('SUMMED_ttleptonic.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['wjets'] = np.genfromtxt('SUMMED_Wjets.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['data'] = np.genfromtxt('SUMMED_collision.csv', delimiter=',', names=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc42d14-e3c4-4e6b-ada2-1cbc5806a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_gen = {'signal':527218.0, \n",
    "         'tt_semilep':143553998.0, \n",
    "         'tt_had': 100522516.0, \n",
    "         'tt_lep':41777512.0, \n",
    "         'wjets':80958227.0,\n",
    "         'data':0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d9712-602f-423c-9e2c-0db340b9e951",
   "metadata": {},
   "source": [
    "## Open correctionlib files\n",
    "\n",
    "Now we will load the two correctionlib JSON files that we used in the earlier examples, and access the specific corrections that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71955de-b939-4ce2-9a97-41fe73fe7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open(\"POG/LUM/2016postVFP_UL/puWeights.json.gz\",'rt') as file:\n",
    "    data = file.read().strip()\n",
    "    evaluator = correctionlib._core.CorrectionSet.from_string(data)\n",
    "with gzip.open(\"POG/MUO/2016postVFP_UL/muon_Z.json.gz\",'rt') as file:\n",
    "    data = file.read().strip()\n",
    "    evaluatorMU = correctionlib._core.CorrectionSet.from_string(data)\n",
    "    \n",
    "pucorr = evaluator[\"Collisions16_UltraLegacy_goldenJSON\"]\n",
    "mucorr = evaluatorMU[\"NUM_TightID_DEN_TrackerMuons\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff8822-c0e9-4d71-ada9-7abdaedf3575",
   "metadata": {},
   "source": [
    "## Store data for histograms\n",
    "\n",
    "Now we will use the data we read from the .csv files to evaluate the 2 corrections and their uncertainties. We will slim down the number of variables that we need to create our final Z' mass histograms and put everything in a final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37ec41e-b12d-4ab5-9b7b-227028a15fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "histoData = {}\n",
    "for sample in datadict.keys():\n",
    "    histoData[sample] = {\n",
    "        \"N_gen\": N_gen[sample],\n",
    "        \"genWeight\": datadict[sample]['weight']/np.abs(datadict[sample]['weight']),\n",
    "        \"mtt\": datadict[sample]['mtt'],\n",
    "        \"pu_weight\": [pucorr.evaluate(n,\"nominal\") for n in datadict[sample]['pileup']],\n",
    "        \"pu_weight_up\": [pucorr.evaluate(n,\"up\") for n in datadict[sample]['pileup']],\n",
    "        \"pu_weight_dn\": [pucorr.evaluate(n,\"down\") for n in datadict[sample]['pileup']],\n",
    "        \"muId_weight\": [mucorr.evaluate(eta,pt,\"nominal\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])],\n",
    "        \"muId_weight_up\": [mucorr.evaluate(eta,pt,\"systup\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])],\n",
    "        \"muId_weight_dn\": [mucorr.evaluate(eta,pt,\"systdown\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])]\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79914572-a209-4282-a37c-3d512e74d399",
   "metadata": {},
   "source": [
    "### Save histogram data \n",
    "\n",
    "We need to open this histogram data in ROOT, so we will move over to our other docker container. Let's write `histoData` in a pickle file that we can copy to our other folder. When you're done with this notebook, go back to the lesson page for the next script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da37f655-a3fd-4999-80a4-b557407e45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hists_for_ROOT.p','wb') as f:\n",
    "    pickle.dump(histoData,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd18ee-f3c0-44bf-a2c5-4b04e8968a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
